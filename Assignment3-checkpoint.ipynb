{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data_Augmentation on Scrapped images from Amazon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "453images loaded\n"
     ]
    }
   ],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile,join\n",
    "\n",
    "mypath=\"./Desktop/FlipRobo/My_datasets/\"\n",
    "file_names=[f for f in listdir(mypath)if isfile(join(mypath,f))]\n",
    "print(str(len(file_names))+'images loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "shirts_count=0\n",
    "Tshirts_count=0\n",
    "sarees_count=0\n",
    "\n",
    "training_size=100\n",
    "test_size=50\n",
    "\n",
    "training_images=[]\n",
    "training_labels=[]\n",
    "test_images=[]\n",
    "test_labels=[]\n",
    "\n",
    "size=150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "shirts_train_dir='./FlipRobo/data/train/shirts/'\n",
    "Tshirts_train_dir='./FlipRobo/data/train/Tshirts/'\n",
    "sarees_train_dir='./FlipRobo/data/train/sarees/'\n",
    "\n",
    "shirts_test_dir='./FlipRobo/data/test/shirts/'\n",
    "Tshirts_test_dir='./FlipRobo/data/test/Tshirts/'\n",
    "sarees_test_dir='./FlipRobo/data/test/sarees/'\n",
    "\n",
    "def make_dir(directory):\n",
    "    if os.path.exists(directory):\n",
    "        shutil.rmtree(directory)\n",
    "    os.makedirs(directory)\n",
    "    \n",
    "make_dir(shirts_train_dir)\n",
    "make_dir(Tshirts_train_dir)\n",
    "make_dir(sarees_train_dir)\n",
    "make_dir(shirts_test_dir)\n",
    "make_dir(Tshirts_test_dir)\n",
    "make_dir(sarees_test_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extraction of Training and Test Data\n",
    "def getzeros(number):\n",
    "    if (number> 10 and number<100):\n",
    "        return \"0\"\n",
    "    if (number<10):\n",
    "        return \"00\"\n",
    "    else :\n",
    "        return \"\"\n",
    "for i ,file in enumerate(file_names):\n",
    "    \n",
    "    if file_names[i][0]==\"s\":\n",
    "        shirts_count+=1\n",
    "        image=cv2.imread(mypath+file)\n",
    "        image=cv2.resize(image,(size,size),interpolation=cv2.INTER_AREA)\n",
    "        if shirts_count<=training_size:\n",
    "            training_images.append(image)\n",
    "            training_labels.append(1)\n",
    "            zeros=getzeros(shirts_count)\n",
    "            cv2.imwrite(shirts_train_dir+\"shirts\"+str(zeros)+str(shirts_count)+\".jpg\",image)\n",
    "        if shirts_count>training_size and shirts_count<=training_size+test_size:\n",
    "            test_images.append(image)\n",
    "            test_labels.append(1)\n",
    "            zeros=getzeros(shirts_count-100)\n",
    "            cv2.imwrite(shirts_test_dir+\"shirts\"+str(zeros)+str(shirts_count)+\".jpg\",image)\n",
    "    if file_names[i][0]==\"T\":\n",
    "            Tshirts_count+=1\n",
    "            image=cv2.imread(mypath+file)\n",
    "            image=cv2.resize(image,(size,size),interpolation=cv2.INTER_AREA)\n",
    "            if Tshirts_count<=training_size:\n",
    "                training_images.append(image)\n",
    "                training_labels.append(1)\n",
    "                zeros=getzeros(Tshirts_count)\n",
    "                cv2.imwrite(Tshirts_train_dir+\"Tshirts\"+str(zeros)+str(Tshirts_count)+\".jpg\",image)\n",
    "            if Tshirts_count>training_size and Tshirts_count<=training_size+test_size:\n",
    "                test_images.append(image)\n",
    "                test_labels.append(1)\n",
    "                zeros=getzeros(Tshirts_count-100)\n",
    "                cv2.imwrite(Tshirts_test_dir+\"Tshirts\"+str(zeros)+str(Tshirts_count)+\".jpg\",image)\n",
    "    if file_names[i][0]==\"S\":\n",
    "        sarees_count+=1\n",
    "        image=cv2.imread(mypath+file)\n",
    "        image=cv2.resize(image,(size,size),interpolation=cv2.INTER_AREA)\n",
    "        if sarees_count<=training_size:\n",
    "            training_images.append(image)\n",
    "            training_labels.append(1)\n",
    "            zeros=getzeros(sarees_count)\n",
    "            cv2.imwrite(sarees_train_dir+\"Sarees\"+str(zeros)+str(sarees_count)+\".jpg\",image)\n",
    "        if sarees_count>training_size and sarees_count<=training_size+test_size:\n",
    "            test_images.append(image)\n",
    "            test_labels.append(1)\n",
    "            zeros=getzeros(sarees_count-100)\n",
    "            cv2.imwrite(sarees_test_dir+\"Sarees\"+str(zeros)+str(sarees_count)+\".jpg\",image)\n",
    "    \n",
    "    if shirts_count==training_size+test_size and Tshirts_count==training_size+test_size and sarees_count==training_size+test_size:\n",
    "            break\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save our loaded data as npz files\n",
    "np.savez(\"Amazon_training_data.npz\",np.array(training_images))\n",
    "np.savez(\"Amazon_training_labels.npz\",np.array(training_labels))\n",
    "np.savez(\"Amazon_test_data.npz\",np.array(test_images))\n",
    "np.savez(\"Amazon_test_labels.npz\",np.array(test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def load_data(datasetname):\n",
    "    npzfile=np.load(\"Amazon_training_data.npz\")\n",
    "    train=npzfile['arr_0']\n",
    "    \n",
    "    npzfile=np.load(\"Amazon_training_labels.npz\")\n",
    "    train_labels=npzfile['arr_0']\n",
    "    \n",
    "    npzfile=np.load(\"Amazon_test_data.npz\")\n",
    "    test=npzfile['arr_0']\n",
    "    \n",
    "    npzfile=np.load(\"Amazon_test_labels.npz\")\n",
    "    test_labels=npzfile['arr_0']\n",
    "    return (train,train_labels),(test,test_labels)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-Tshirts\n",
      "2-Tshirts\n",
      "3-Tshirts\n",
      "4-Tshirts\n"
     ]
    }
   ],
   "source": [
    "#show the loaded data\n",
    "for i in range(1,5):\n",
    "    random=np.random.randint(0,len(training_images))\n",
    "    cv2.imshow(\"image_\"+str(i),training_images[random])\n",
    "    if training_labels[random]==0:\n",
    "        print(str(i)+\"-shirts\")\n",
    "    elif training_labels[random]==1:\n",
    "           print(str(i)+\"-Tshirts\")\n",
    "    else:\n",
    "        print(str(i)+\"-sarees\")\n",
    "    cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300, 150, 150, 3)\n",
      "(300, 1)\n",
      "(120, 150, 150, 3)\n",
      "(120, 1)\n"
     ]
    }
   ],
   "source": [
    "#make format that accepted by Keras\n",
    "(x_train,y_train),(x_test,y_test)=load_data('Amazon')\n",
    "y_train=y_train.reshape(y_train.shape[0],1)\n",
    "y_test=y_test.reshape(y_test.shape[0],1)\n",
    "\n",
    "x_train=x_train.astype('float32')\n",
    "x_test=x_test.astype('float32')\n",
    "\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\hp\\anaconda3\\envs\\Virtual\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:17: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#build a model\n",
    "#importing the libraries\n",
    "\n",
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,Dropout,Activation,Flatten,Conv2D,MaxPooling2D\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from keras import optimizers\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_rows=x_train[0].shape[0]\n",
    "img_cols=x_train[1].shape[0]\n",
    "input_shape=(img_rows,img_cols,3)\n",
    "\n",
    "batch_size=32\n",
    "epochs=30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\hp\\anaconda3\\envs\\Virtual\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 148, 148, 32)      896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 74, 74, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 74, 74, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 72, 72, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 36, 36, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 36, 36, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 34, 34, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 17, 17, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 17, 17, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 18496)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               2367616   \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 3)                 387       \n",
      "=================================================================\n",
      "Total params: 2,396,643\n",
      "Trainable params: 2,396,643\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model=Sequential()\n",
    "\n",
    "model.add(Conv2D(32,(3,3),input_shape=input_shape,activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Conv2D(32,(3,3),activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Conv2D(64,(3,3),activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.2))\n",
    "    \n",
    "model.add(Flatten())\n",
    "model.add(Dense(128,activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(3,activation='softmax'))\n",
    "    \n",
    "print(model.summary())\n",
    "  \n",
    "model.compile(optimizer='adam',metrics=['accuracy'],loss='sparse_categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 300 samples, validate on 120 samples\n",
      "Epoch 1/30\n",
      "300/300 [==============================] - 15s 51ms/sample - loss: 0.1401 - acc: 0.8967 - val_loss: 4.4306e-07 - val_acc: 1.0000\n",
      "Epoch 2/30\n",
      "300/300 [==============================] - 15s 50ms/sample - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 2.9802e-09 - val_acc: 1.0000\n",
      "Epoch 3/30\n",
      "300/300 [==============================] - 15s 50ms/sample - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 4/30\n",
      "300/300 [==============================] - 16s 53ms/sample - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 5/30\n",
      "300/300 [==============================] - 15s 49ms/sample - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 6/30\n",
      "300/300 [==============================] - 14s 48ms/sample - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 7/30\n",
      "300/300 [==============================] - 15s 49ms/sample - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 8/30\n",
      "300/300 [==============================] - 16s 52ms/sample - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 9/30\n",
      "300/300 [==============================] - 16s 52ms/sample - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 10/30\n",
      "300/300 [==============================] - 15s 51ms/sample - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 11/30\n",
      "300/300 [==============================] - 15s 50ms/sample - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 12/30\n",
      "300/300 [==============================] - 15s 49ms/sample - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 13/30\n",
      "300/300 [==============================] - 15s 49ms/sample - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 14/30\n",
      "300/300 [==============================] - 14s 47ms/sample - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 15/30\n",
      "300/300 [==============================] - 15s 50ms/sample - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 16/30\n",
      "300/300 [==============================] - 15s 51ms/sample - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 17/30\n",
      "300/300 [==============================] - 14s 48ms/sample - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 18/30\n",
      "300/300 [==============================] - 15s 50ms/sample - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 19/30\n",
      "300/300 [==============================] - 15s 50ms/sample - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 20/30\n",
      "300/300 [==============================] - 15s 49ms/sample - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 21/30\n",
      "300/300 [==============================] - 15s 49ms/sample - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 22/30\n",
      "300/300 [==============================] - 15s 50ms/sample - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 23/30\n",
      "300/300 [==============================] - 15s 50ms/sample - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 24/30\n",
      "300/300 [==============================] - 15s 50ms/sample - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 25/30\n",
      "300/300 [==============================] - 15s 50ms/sample - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 26/30\n",
      "300/300 [==============================] - 15s 50ms/sample - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 27/30\n",
      "300/300 [==============================] - 15s 50ms/sample - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 28/30\n",
      "300/300 [==============================] - 16s 52ms/sample - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 29/30\n",
      "300/300 [==============================] - 15s 50ms/sample - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 30/30\n",
      "300/300 [==============================] - 17s 55ms/sample - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "120/120 [==============================] - 2s 13ms/sample - loss: 0.0000e+00 - acc: 1.0000\n",
      "Test loss: 0.0\n",
      "Test Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "history=model.fit(x_train,y_train,\n",
    "            batch_size=batch_size,\n",
    "            epochs=epochs,\n",
    "            validation_data=(x_test,y_test),\n",
    "            shuffle=True)\n",
    "\n",
    "scores=model.evaluate(x_test,y_test,verbose=1)\n",
    "print('Test loss:',scores[0])\n",
    "print('Test Accuracy:',scores[1])\n",
    "\n",
    "model.save('Amazon_preTest_augmentation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "453 images loaded\n"
     ]
    }
   ],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile,join\n",
    "mypath=\".\\\\Desktop\\\\FlipRobo\\\\My_datasets\\\\\"\n",
    "file_names=[f for f in listdir(mypath)if isfile(join(mypath,f))]\n",
    "print(str(len(file_names))+' images loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sharpened images for shirts\n",
    "shirts_count=0\n",
    "for i,file in enumerate(file_names):\n",
    "    if file_names[i][0]==\"s\":\n",
    "        shirts_count+=1\n",
    "        images=cv2.imread(\".\\\\Desktop\\\\FlipRobo\\\\My_datasets\\\\\"+file)\n",
    "        images=cv2.resize(image,(150,150),cv2.INTER_AREA)\n",
    "        kernel_sharpening=np.array([[-1,-1,-1],\n",
    "                                    [-1,9,-1],\n",
    "                                    [-1,-1,-1]])\n",
    "        sharpened=cv2.filter2D(images,-1,kernel_sharpening)\n",
    "        cv2.imwrite(\".\\\\Desktop\\\\data\\\\augmented\\\\shirts\\\\shirts_sharpened\"+file,sharpened)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#flipped images for shirts\n",
    "shirts_count=0\n",
    "for i,file in enumerate(file_names):\n",
    "        if file_names[i][0]==\"s\":\n",
    "            shirts_count+=1\n",
    "            images=cv2.imread(\".\\\\Desktop\\\\FlipRobo\\\\My_datasets\\\\\"+file)\n",
    "        #flip image horizontally\n",
    "            flipped=cv2.flip(images,1)\n",
    "            cv2.imwrite(\".\\\\Desktop\\\\data\\\\augmented\\\\shirts\\\\shirts_flipped\"+file,flipped)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rotation of the image\n",
    "shirts_count=0\n",
    "for i,file in enumerate(file_names):\n",
    "        if file_names[i][0]==\"s\":\n",
    "            shirts_count+=1\n",
    "            images=cv2.imread(\".\\\\Desktop\\\\FlipRobo\\\\My_datasets\\\\\"+file)\n",
    "        \n",
    "            height,width=images.shape[:2]\n",
    "            rotation_matrix=cv2.getRotationMatrix2D((width/2,height/2),(30),1)\n",
    "            rotated_image=cv2.warpAffine(images,rotation_matrix,(width,height))\n",
    "            cv2.imwrite(\".\\\\Desktop\\\\data\\\\augmented\\\\shirts\\\\shirts_rotated\"+file,rotated_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#increasing Brightness for shirts\n",
    "shirts_count=0\n",
    "for i,file in enumerate(file_names):\n",
    "        if file_names[i][0]==\"s\":\n",
    "            shirts_count+=1\n",
    "        \n",
    "            images=cv2.imread(\".\\\\Desktop\\\\FlipRobo\\\\My_datasets\\\\\"+file)\n",
    "        \n",
    "            m=np.ones(images.shape,dtype='uint8')*75\n",
    "            added=cv2.add(images,m)\n",
    "            cv2.imwrite(\".\\\\Desktop\\\\data\\\\augmented\\\\shirts\\\\shirts_brightness_increase\"+file,added)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#increasing Brightness for sarees\n",
    "sarees_count=0\n",
    "for i,file in enumerate(file_names):\n",
    "        if file_names[i][0]==\"S\":\n",
    "            sarees_count+=1\n",
    "        \n",
    "            images=cv2.imread(\".\\\\Desktop\\\\FlipRobo\\\\My_datasets\\\\\"+file)\n",
    "            \n",
    "            m=np.ones(images.shape,dtype='uint8')*75\n",
    "            added=cv2.add(images,m)\n",
    "            cv2.imwrite(\".\\\\Desktop\\\\data\\\\augmented\\\\sarees\\\\sarees_brightness_increase\"+file,added)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rotation of the image for sarees\n",
    "sarees_count=0\n",
    "for i,file in enumerate(file_names):\n",
    "        if file_names[i][0]==\"S\":\n",
    "            sarees_count+=1\n",
    "            images=cv2.imread(\".\\\\Desktop\\\\FlipRobo\\\\My_datasets\\\\\"+file)\n",
    "        \n",
    "            height,width=images.shape[:2]\n",
    "            rotation_matrix=cv2.getRotationMatrix2D((width/2,height/2),(30),1)\n",
    "            rotated_image=cv2.warpAffine(images,rotation_matrix,(width,height))\n",
    "            cv2.imwrite(\".\\\\Desktop\\\\data\\\\augmented\\\\sarees\\\\sarees_rotated\"+file,rotated_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#flipped images for sarees\n",
    "sarees_count=0\n",
    "for i,file in enumerate(file_names):\n",
    "        if file_names[i][0]==\"S\":\n",
    "            sarees_count+=1\n",
    "            images=cv2.imread(\".\\\\Desktop\\\\FlipRobo\\\\My_datasets\\\\\"+file)\n",
    "        #flip image horizontally\n",
    "            flipped=cv2.flip(images,1)\n",
    "            cv2.imwrite(\".\\\\Desktop\\\\data\\\\augmented\\\\sarees\\\\sarees_flipped\"+file,flipped)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sharpened images for sarees\n",
    "sarees_count=0\n",
    "for i,file in enumerate(file_names):\n",
    "    if file_names[i][0]==\"S\":\n",
    "        sarees_count+=1\n",
    "        images=cv2.imread(\".\\\\Desktop\\\\FlipRobo\\\\My_datasets\\\\\"+file)\n",
    "        images=cv2.resize(image,(150,150),cv2.INTER_AREA)\n",
    "        kernel_sharpening=np.array([[-1,-1,-1],\n",
    "                                    [-1,9,-1],\n",
    "                                    [-1,-1,-1]])\n",
    "        sharpened=cv2.filter2D(images,-1,kernel_sharpening)\n",
    "        cv2.imwrite(\".\\\\Desktop\\\\data\\\\augmented\\\\sarees\\\\sarees_sharpened\"+file,sharpened)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sharpened images for Tshirts\n",
    "Tshirts_count=0\n",
    "for i,file in enumerate(file_names):\n",
    "    if file_names[i][0]==\"T\":\n",
    "        Tshirts_count+=1\n",
    "        images=cv2.imread(\".\\\\Desktop\\\\FlipRobo\\\\My_datasets\\\\\"+file)\n",
    "        images=cv2.resize(image,(150,150),cv2.INTER_AREA)\n",
    "        kernel_sharpening=np.array([[-1,-1,-1],\n",
    "                                    [-1,9,-1],\n",
    "                                    [-1,-1,-1]])\n",
    "        sharpened=cv2.filter2D(images,-1,kernel_sharpening)\n",
    "        cv2.imwrite(\".\\\\Desktop\\\\data\\\\augmented\\\\tshirts\\\\tshirts_sharpened\"+file,sharpened)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#flipped images for Tshirts\n",
    "Tshirts_count=0\n",
    "for i,file in enumerate(file_names):\n",
    "        if file_names[i][0]==\"T\":\n",
    "            Tshirts_count+=1\n",
    "            images=cv2.imread(\".\\\\Desktop\\\\FlipRobo\\\\My_datasets\\\\\"+file)\n",
    "        #flip image horizontally\n",
    "            flipped=cv2.flip(images,1)\n",
    "            cv2.imwrite(\".\\\\Desktop\\\\data\\\\augmented\\\\tshirts\\\\tshirts_flipped\"+file,flipped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rotation of the image for Tshirts\n",
    "Tshirts_count=0\n",
    "for i,file in enumerate(file_names):\n",
    "        if file_names[i][0]==\"T\":\n",
    "            Tshirts_count+=1\n",
    "            images=cv2.imread(\".\\\\Desktop\\\\FlipRobo\\\\My_datasets\\\\\"+file)\n",
    "        \n",
    "            height,width=images.shape[:2]\n",
    "            rotation_matrix=cv2.getRotationMatrix2D((width/2,height/2),(30),1)\n",
    "            rotated_image=cv2.warpAffine(images,rotation_matrix,(width,height))\n",
    "            cv2.imwrite(\".\\\\Desktop\\\\data\\\\augmented\\\\tshirts\\\\tshirts_rotated\"+file,rotated_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#increasing Brightness\n",
    "Tshirts_count=0\n",
    "for i,file in enumerate(file_names):\n",
    "        if file_names[i][0]==\"T\":\n",
    "            Tshirts_count+=1\n",
    "        \n",
    "            images=cv2.imread(\".\\\\Desktop\\\\FlipRobo\\\\My_datasets\\\\\"+file)\n",
    "            \n",
    "            m=np.ones(images.shape,dtype='uint8')*75\n",
    "            added=cv2.add(images,m)\n",
    "            cv2.imwrite(\".\\\\Desktop\\\\data\\\\augmented\\\\tshirts\\\\tshirts_brightness_increased\"+file,added)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1812 images loaded\n"
     ]
    }
   ],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile,join\n",
    "mypath=\"C:\\\\Users\\\\hp\\\\Desktop\\\\amazon\\\\\"\n",
    "file_names=[f for f in listdir(mypath)if isfile(join(mypath,f))]\n",
    "print(str(len(file_names))+' images loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import shutil\n",
    "shirts_count=0\n",
    "Tshirts_count=0\n",
    "sarees_count=0\n",
    "training_size=500\n",
    "test_size=150\n",
    "training_images=[]\n",
    "training_labels=[]\n",
    "test_images=[]\n",
    "test_labels=[]\n",
    "size=150\n",
    "shirts_dir_train=\"C:\\\\data\\Train\\\\tshirts\"\n",
    "tshirts_dir_train=\"C:\\\\data\\Train\\\\tshirts\"\n",
    "sarees_dir_train=\"C:\\\\data\\\\Train\\\\sarees\"\n",
    "shirts_dir_val=\"C:\\\\data\\\\Validation\\shirts\"\n",
    "tshirts_dir_val=\"C:\\\\data\\Validation\\\\tshirts\"\n",
    "sarees_dir_val=\"C:\\\\data\\\\Validation\\\\sarees\"\n",
    "def make_dir(directory):\n",
    "    if os.path.exists(directory):\n",
    "        shutil.rmtree(directory)\n",
    "    os.makedirs(directory)\n",
    "make_dir(shirts_dir_train)\n",
    "make_dir(tshirts_dir_train)\n",
    "make_dir(sarees_dir_train)\n",
    "make_dir(shirts_dir_val)\n",
    "make_dir(tshirts_dir_val)\n",
    "make_dir(sarees_dir_val)\n",
    "def getzeros(number):\n",
    "    if (number> 10 and number<400):\n",
    "        return \"0\"\n",
    "    if (number<10):\n",
    "        return \"00\"\n",
    "    else :\n",
    "        return \"\"\n",
    "for i ,file in enumerate(file_names):\n",
    "    if file_names[i][0]==\"s\":\n",
    "        shirts_count+=1\n",
    "        image=cv2.imread(\"C:\\\\Users\\\\hp\\\\Desktop\\\\amazon\\\\\"+file)\n",
    "        image=cv2.resize(image,(150,150),cv2.INTER_AREA)\n",
    "        if shirts_count<=training_size:\n",
    "            training_images.append(image)\n",
    "            training_labels.append(1)\n",
    "            zeros=getzeros(shirts_count)\n",
    "            cv2.imwrite(shirts_dir_train+\"shirts\"+str(zeros)+str(shirts_count)+\".jpg\",image)\n",
    "        if shirts_count>training_size and shirts_count<=training_size+test_size:\n",
    "            test_images.append(image)\n",
    "            test_labels.append(1)\n",
    "            zeros=getzeros(shirts_count-400)\n",
    "            cv2.imwrite(shirts_dir_val+\"shirts\"+str(zeros)+str(shirts_count)+\".jpg\",image)\n",
    "    if file_names[i][0]==\"T\":\n",
    "            Tshirts_count+=1\n",
    "            image=cv2.imread(\"C:\\\\Users\\\\hp\\\\Desktop\\\\amazon\\\\\"+file)\n",
    "            image=cv2.resize(image,(150,150),cv2.INTER_AREA)\n",
    "            if Tshirts_count<=training_size:\n",
    "                training_images.append(image)\n",
    "                training_labels.append(1)\n",
    "                zeros=getzeros(Tshirts_count)\n",
    "                cv2.imwrite(tshirts_dir_train+\"tshirts\"+str(zeros)+str(tshirts_count)+\".jpg\",image)\n",
    "            if Tshirts_count>training_size and Tshirts_count<=training_size+test_size:\n",
    "                test_images.append(image)\n",
    "                test_labels.append(1)\n",
    "                zeros=getzeros(Tshirts_count-400)\n",
    "            cv2.imwrite(tshirts_dir_val+\"tshirts\"+str(zeros)+str(tshirts_count)+\".jpg\",image)\n",
    "    if file_names[i][0]==\"S\":\n",
    "        sarees_count+=1\n",
    "        image=cv2.imread(\"C:\\\\Users\\\\hp\\\\Desktop\\\\amazon\\\\\"+file)\n",
    "        image=cv2.resize(image,(150,150),cv2.INTER_AREA)\n",
    "        if sarees_count<=training_size:\n",
    "            training_images.append(image)\n",
    "            training_labels.append(1)\n",
    "            zeros=getzeros(sarees_count)\n",
    "            cv2.imwrite(sarees_dir_train+\"sarees\"+str(zeros)+str(sarees_count)+\".jpg\",image)\n",
    "        if sarees_count>training_size and sarees_count<=training_size+test_size:\n",
    "            test_images.append(image)\n",
    "            test_labels.append(1)\n",
    "            zeros=getzeros(sarees_count-400)\n",
    "            cv2.imwrite(sarees_dir_val+\"sarees\"+str(zeros)+str(sarees_count)+\".jpg\",image)\n",
    "    if shirts_count==training_size+test_size and Tshirts_count==training_size+test_size and sarees_count==training_size+test_size:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez(\"amazon_augmented_training_data.npz\",np.array(training_images))\n",
    "np.savez(\"amazon_augmented_training_labels.npz\",np.array(training_labels))\n",
    "np.savez(\"amazon_augmented_test_data.npz\",np.array(test_images))\n",
    "np.savez(\"amazon_augmented_test_labels.npz\",np.array(test_labels))       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def load_data(datasetname):\n",
    "    npzfile=np.load(\"amazon_augmented_training_data.npz\")\n",
    "    train=npzfile['arr_0']\n",
    "    npzfile=np.load(\"amazon_augmented_training_labels.npz\")\n",
    "    train_labels=npzfile['arr_0']\n",
    "    npzfile=np.load(\"amazon_augmented_test_data.npz\")\n",
    "    test=npzfile['arr_0']\n",
    "    npzfile=np.load(\"amazon_augmented_test_labels.npz\")\n",
    "    test_labels=npzfile['arr_0']\n",
    "    return (train,train_labels),(test,test_labels)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-sarees\n",
      "2-sarees\n",
      "3-sarees\n",
      "4-sarees\n",
      "5-sarees\n",
      "6-sarees\n",
      "7-sarees\n",
      "8-sarees\n",
      "9-sarees\n",
      "10-sarees\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,11):\n",
    "    random=np.random.randint(0,len(training_images))\n",
    "    cv2.imshow(\"image_\"+str(i),training_images[random])\n",
    "    if training_labels[random]==0:\n",
    "        print(str(i)+\"-shirts\")\n",
    "    if training_labels[random]==1:\n",
    "        print(str(i)+\"-sarees\")\n",
    "    else:\n",
    "        print(str(i)+\"-Tshirts\")\n",
    "    cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 150, 150, 3)\n",
      "(150, 150, 150, 3)\n",
      "(500, 1)\n",
      "(150, 1)\n"
     ]
    }
   ],
   "source": [
    "(x_train,y_train),(x_test,y_test)=load_data(\"amazonaugmented\")\n",
    "y_train=y_train.reshape(y_train.shape[0],1)\n",
    "y_test=y_test.reshape(y_test.shape[0],1)\n",
    "x_train=x_train.astype(\"float32\")\n",
    "x_test=x_test.astype(\"float32\")\n",
    "x_train/=255\n",
    "x_test/=255\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_3 (Conv2D)            (None, 148, 148, 32)      896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 74, 74, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 74, 74, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 72, 72, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 36, 36, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 36, 36, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 34, 34, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 17, 17, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 17, 17, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 18496)             0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                1183808   \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 3)                 195       \n",
      "=================================================================\n",
      "Total params: 1,212,643\n",
      "Trainable params: 1,212,643\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#build the model with augmented datasets\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,Dropout,Activation,Flatten,Conv2D,MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "from keras import optimizers\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.utils import plot_model\n",
    "import os\n",
    "batch_size=16\n",
    "input_shape=(150,150,3)\n",
    "epochs=10\n",
    "img_rows=x_train[0].shape[0]\n",
    "img_cols=x_train[1].shape[0]\n",
    "\n",
    "model=Sequential()\n",
    "\n",
    "model.add(Conv2D(32,(3,3),input_shape=input_shape,activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Conv2D(32,(3,3),activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Conv2D(64,(3,3),activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.2))\n",
    "    \n",
    "model.add(Flatten())\n",
    "model.add(Dense(64,activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(3,activation='softmax'))\n",
    "    \n",
    "print(model.summary())\n",
    "  \n",
    "model.compile(optimizer='adam',metrics=['accuracy'],loss='sparse_categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 500 samples, validate on 150 samples\n",
      "Epoch 1/10\n",
      "500/500 [==============================] - 26s 52ms/sample - loss: 0.0257 - acc: 0.9940 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 2/10\n",
      "500/500 [==============================] - 24s 48ms/sample - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 3/10\n",
      "500/500 [==============================] - 24s 48ms/sample - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 4/10\n",
      "500/500 [==============================] - 24s 48ms/sample - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 5/10\n",
      "500/500 [==============================] - 24s 48ms/sample - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 6/10\n",
      "500/500 [==============================] - 24s 48ms/sample - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 7/10\n",
      "500/500 [==============================] - 25s 49ms/sample - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 8/10\n",
      "500/500 [==============================] - 24s 48ms/sample - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 9/10\n",
      "500/500 [==============================] - 25s 49ms/sample - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 10/10\n",
      "500/500 [==============================] - 24s 48ms/sample - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "150/150 [==============================] - 2s 10ms/sample - loss: 0.0000e+00 - acc: 1.0000\n",
      "Test loss: 0.0\n",
      "Test Accuracy 1.0\n"
     ]
    }
   ],
   "source": [
    "history=model.fit(x_train,y_train,batch_size=batch_size,epochs=epochs,validation_data=(x_test,y_test),shuffle=True)\n",
    "model.save(\"augmented_datasets\")\n",
    "scores=model.evaluate(x_test,y_test,verbose=1)\n",
    "print('Test loss:',scores[0])\n",
    "print(\"Test Accuracy\",scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plotting accuracy chart\n",
    "\n",
    "history_dict=history.history\n",
    "\n",
    "loss_values=history_dict['acc']\n",
    "val_loss_values=history_dict['val_acc']\n",
    "epochs=range(1,len(loss_values)+1)\n",
    "\n",
    "line1=plt.plot(epochs,val_loss_values,label='Validation Accuracy')\n",
    "line2=plt.plot(epochs,loss_values,label='Training Accuracy')\n",
    "\n",
    "plt.setp(line1,linewidth=1.0,marker='+',markersize=10.0)\n",
    "plt.setp(line2,linewidth=1.0,marker='4',markersize=10.0)\n",
    "\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
