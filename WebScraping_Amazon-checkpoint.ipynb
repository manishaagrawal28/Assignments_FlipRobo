{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the libraries\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import StaleElementReferenceException\n",
    "import os\n",
    "import shutil\n",
    "import requests\n",
    "\n",
    "#getting the url of webpage\n",
    "\n",
    "driver=webdriver.Chrome('C:\\\\Users\\\\hp\\\\Downloads\\\\chromedriver_win32\\\\chromedriver.exe')\n",
    "my_page=driver.get('https://www.amazon.in/gp/browse.html?node=1968093031&ref_=nav_em_sbc_mfashion_shirts_0_2_9_4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make directory to store scrapped images\n",
    "\n",
    "def make_directory(dirname):\n",
    "    current_path=os.getcwd()\n",
    "    path=os.path.join(current_path,dirname)\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_directory('Amazon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scraping images of further pages\n",
    "\n",
    "start_page=1\n",
    "total_pages=3\n",
    "\n",
    "def scrap_image_url(driver):\n",
    "    images=driver.find_elements_by_xpath(\"//div[@class='a-row a-spacing-base']//img\")\n",
    "    product_data= {}\n",
    "    product_data['image_urls']=[]\n",
    "    for image in images:\n",
    "        source=image.get_attribute('src')\n",
    "        product_data['image_urls'].append(source)\n",
    "    return product_data\n",
    "\n",
    "\n",
    "def save_images(data,dirname,page):\n",
    "    for index,link in enumerate(data['image_urls']):\n",
    "        print('Downloading {0} of {1} images'.format(index+1,len(data['image_urls'])))\n",
    "        response=requests.get(link)\n",
    "        with open('{0}/img_{1}{2}.jpeg'.format(dirname,page,index),'wb') as file:\n",
    "            file.write(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page 1 of 3 pages\n",
      "The Current Page Scrapped is 2\n",
      "Downloading 1 of 48 images\n",
      "Downloading 2 of 48 images\n",
      "Downloading 3 of 48 images\n",
      "Downloading 4 of 48 images\n",
      "Downloading 5 of 48 images\n",
      "Downloading 6 of 48 images\n",
      "Downloading 7 of 48 images\n",
      "Downloading 8 of 48 images\n",
      "Downloading 9 of 48 images\n",
      "Downloading 10 of 48 images\n",
      "Downloading 11 of 48 images\n",
      "Downloading 12 of 48 images\n",
      "Downloading 13 of 48 images\n",
      "Downloading 14 of 48 images\n",
      "Downloading 15 of 48 images\n",
      "Downloading 16 of 48 images\n",
      "Downloading 17 of 48 images\n",
      "Downloading 18 of 48 images\n",
      "Downloading 19 of 48 images\n",
      "Downloading 20 of 48 images\n",
      "Downloading 21 of 48 images\n",
      "Downloading 22 of 48 images\n",
      "Downloading 23 of 48 images\n",
      "Downloading 24 of 48 images\n",
      "Downloading 25 of 48 images\n",
      "Downloading 26 of 48 images\n",
      "Downloading 27 of 48 images\n",
      "Downloading 28 of 48 images\n",
      "Downloading 29 of 48 images\n",
      "Downloading 30 of 48 images\n",
      "Downloading 31 of 48 images\n",
      "Downloading 32 of 48 images\n",
      "Downloading 33 of 48 images\n",
      "Downloading 34 of 48 images\n",
      "Downloading 35 of 48 images\n",
      "Downloading 36 of 48 images\n",
      "Downloading 37 of 48 images\n",
      "Downloading 38 of 48 images\n",
      "Downloading 39 of 48 images\n",
      "Downloading 40 of 48 images\n",
      "Downloading 41 of 48 images\n",
      "Downloading 42 of 48 images\n",
      "Downloading 43 of 48 images\n",
      "Downloading 44 of 48 images\n",
      "Downloading 45 of 48 images\n",
      "Downloading 46 of 48 images\n",
      "Downloading 47 of 48 images\n",
      "Downloading 48 of 48 images\n",
      "Scraping of page 1 done\n",
      "Scraping page 2 of 3 pages\n",
      "The Current Page Scrapped is 2\n",
      "Downloading 1 of 48 images\n",
      "Downloading 2 of 48 images\n",
      "Downloading 3 of 48 images\n",
      "Downloading 4 of 48 images\n",
      "Downloading 5 of 48 images\n",
      "Downloading 6 of 48 images\n",
      "Downloading 7 of 48 images\n",
      "Downloading 8 of 48 images\n",
      "Downloading 9 of 48 images\n",
      "Downloading 10 of 48 images\n",
      "Downloading 11 of 48 images\n",
      "Downloading 12 of 48 images\n",
      "Downloading 13 of 48 images\n",
      "Downloading 14 of 48 images\n",
      "Downloading 15 of 48 images\n",
      "Downloading 16 of 48 images\n",
      "Downloading 17 of 48 images\n",
      "Downloading 18 of 48 images\n",
      "Downloading 19 of 48 images\n",
      "Downloading 20 of 48 images\n",
      "Downloading 21 of 48 images\n",
      "Downloading 22 of 48 images\n",
      "Downloading 23 of 48 images\n",
      "Downloading 24 of 48 images\n",
      "Downloading 25 of 48 images\n",
      "Downloading 26 of 48 images\n",
      "Downloading 27 of 48 images\n",
      "Downloading 28 of 48 images\n",
      "Downloading 29 of 48 images\n",
      "Downloading 30 of 48 images\n",
      "Downloading 31 of 48 images\n",
      "Downloading 32 of 48 images\n",
      "Downloading 33 of 48 images\n",
      "Downloading 34 of 48 images\n",
      "Downloading 35 of 48 images\n",
      "Downloading 36 of 48 images\n",
      "Downloading 37 of 48 images\n",
      "Downloading 38 of 48 images\n",
      "Downloading 39 of 48 images\n",
      "Downloading 40 of 48 images\n",
      "Downloading 41 of 48 images\n",
      "Downloading 42 of 48 images\n",
      "Downloading 43 of 48 images\n",
      "Downloading 44 of 48 images\n",
      "Downloading 45 of 48 images\n",
      "Downloading 46 of 48 images\n",
      "Downloading 47 of 48 images\n",
      "Downloading 48 of 48 images\n",
      "Scraping of page 2 done\n",
      "Scraping page 3 of 3 pages\n",
      "The Current Page Scrapped is 2\n",
      "Downloading 1 of 48 images\n",
      "Downloading 2 of 48 images\n",
      "Downloading 3 of 48 images\n",
      "Downloading 4 of 48 images\n",
      "Downloading 5 of 48 images\n",
      "Downloading 6 of 48 images\n",
      "Downloading 7 of 48 images\n",
      "Downloading 8 of 48 images\n",
      "Downloading 9 of 48 images\n",
      "Downloading 10 of 48 images\n",
      "Downloading 11 of 48 images\n",
      "Downloading 12 of 48 images\n",
      "Downloading 13 of 48 images\n",
      "Downloading 14 of 48 images\n",
      "Downloading 15 of 48 images\n",
      "Downloading 16 of 48 images\n",
      "Downloading 17 of 48 images\n",
      "Downloading 18 of 48 images\n",
      "Downloading 19 of 48 images\n",
      "Downloading 20 of 48 images\n",
      "Downloading 21 of 48 images\n",
      "Downloading 22 of 48 images\n",
      "Downloading 23 of 48 images\n",
      "Downloading 24 of 48 images\n",
      "Downloading 25 of 48 images\n",
      "Downloading 26 of 48 images\n",
      "Downloading 27 of 48 images\n",
      "Downloading 28 of 48 images\n",
      "Downloading 29 of 48 images\n",
      "Downloading 30 of 48 images\n",
      "Downloading 31 of 48 images\n",
      "Downloading 32 of 48 images\n",
      "Downloading 33 of 48 images\n",
      "Downloading 34 of 48 images\n",
      "Downloading 35 of 48 images\n",
      "Downloading 36 of 48 images\n",
      "Downloading 37 of 48 images\n",
      "Downloading 38 of 48 images\n",
      "Downloading 39 of 48 images\n",
      "Downloading 40 of 48 images\n",
      "Downloading 41 of 48 images\n",
      "Downloading 42 of 48 images\n",
      "Downloading 43 of 48 images\n",
      "Downloading 44 of 48 images\n",
      "Downloading 45 of 48 images\n",
      "Downloading 46 of 48 images\n",
      "Downloading 47 of 48 images\n",
      "Downloading 48 of 48 images\n",
      "Scraping of page 3 done\n"
     ]
    }
   ],
   "source": [
    "for page in range(start_page, total_pages+1):\n",
    "    try:\n",
    "        product=scrap_image_url(driver=driver)\n",
    "        print(\"Scraping page {0} of {1} pages\".format(page,total_pages))\n",
    "         \n",
    "        page_value=driver.find_element_by_xpath(\"//span[@class='pagnLink']//a\").text\n",
    "        print('The Current Page Scrapped is {}'.format(page_value))\n",
    "        \n",
    "        #download the images\n",
    "        save_images(data=product,dirname='Amazon',page=page)\n",
    "        print(\"Scraping of page {0} done\".format(page))\n",
    "        \n",
    "    except StaleElementReferenceException as Exception:\n",
    "        print('We are facing an exception')\n",
    "        \n",
    "        exp_page=driver.find_element_by_xpath(\"//span[@class='pagnLink']//a\").text\n",
    "        print('The page value at the time of exception is {}'.format(exp_page))\n",
    "        \n",
    "        value=driver.find_element_by_xpath(\"//span[@class='pagnLink']//a\")\n",
    "        link=value.get_attribute('href')\n",
    "        driver.get(link)\n",
    "        \n",
    "        product=scrap_image_url(driver=driver)\n",
    "        print(\"Scraping page {0} of {1} pages\".format(page,total_pages))\n",
    "         \n",
    "        page_value=driver.find_element_by_xpath(\"//span[@class='pagnLink']//a\").text\n",
    "        print('The Current Page Scrapped is {}'.format(page_value))\n",
    "        \n",
    "        #download the images\n",
    "        save_images(data=product,dirname='Amazon',page=page)\n",
    "        print(\"Scraping of page {0} done\".format(page))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrapped images from amazon(t-shirts(men))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the libraries\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import StaleElementReferenceException\n",
    "import os\n",
    "import shutil\n",
    "import requests\n",
    "\n",
    "#getting the url of webpage\n",
    "\n",
    "driver=webdriver.Chrome('C:\\\\Users\\\\hp\\\\Downloads\\\\chromedriver_win32\\\\chromedriver.exe')\n",
    "my_page=driver.get('https://www.amazon.in/gp/browse.html?node=1968120031&ref_=nav_em_sbc_mfashion_tshirts_0_2_9_3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make directory to store scrapped images\n",
    "\n",
    "def make_directory(dirname):\n",
    "    current_path=os.getcwd()\n",
    "    path=os.path.join(current_path,dirname)\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_directory('Amazon_men_Tshirts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scraping images of further pages\n",
    "\n",
    "start_page=1\n",
    "total_pages=3\n",
    "\n",
    "def scrap_image_url(driver):\n",
    "    images=driver.find_elements_by_xpath(\"//div[@class='a-section aok-relative s-image-tall-aspect']//img\")\n",
    "    product_data= {}\n",
    "    product_data['image_urls']=[]\n",
    "    for image in images:\n",
    "        source=image.get_attribute('src')\n",
    "        product_data['image_urls'].append(source)\n",
    "    return product_data\n",
    "\n",
    "\n",
    "def save_images(data,dirname,page):\n",
    "    for index,link in enumerate(data['image_urls']):\n",
    "        print('Downloading {0} of {1} images'.format(index+1,len(data['image_urls'])))\n",
    "        response=requests.get(link)\n",
    "        with open('{0}/img_{1}{2}.jpeg'.format(dirname,page,index),'wb') as file:\n",
    "            file.write(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page 1 of 3 pages\n",
      "The Current Page Scrapped is 2\n",
      "Downloading 1 of 60 images\n",
      "Downloading 2 of 60 images\n",
      "Downloading 3 of 60 images\n",
      "Downloading 4 of 60 images\n",
      "Downloading 5 of 60 images\n",
      "Downloading 6 of 60 images\n",
      "Downloading 7 of 60 images\n",
      "Downloading 8 of 60 images\n",
      "Downloading 9 of 60 images\n",
      "Downloading 10 of 60 images\n",
      "Downloading 11 of 60 images\n",
      "Downloading 12 of 60 images\n",
      "Downloading 13 of 60 images\n",
      "Downloading 14 of 60 images\n",
      "Downloading 15 of 60 images\n",
      "Downloading 16 of 60 images\n",
      "Downloading 17 of 60 images\n",
      "Downloading 18 of 60 images\n",
      "Downloading 19 of 60 images\n",
      "Downloading 20 of 60 images\n",
      "Downloading 21 of 60 images\n",
      "Downloading 22 of 60 images\n",
      "Downloading 23 of 60 images\n",
      "Downloading 24 of 60 images\n",
      "Downloading 25 of 60 images\n",
      "Downloading 26 of 60 images\n",
      "Downloading 27 of 60 images\n",
      "Downloading 28 of 60 images\n",
      "Downloading 29 of 60 images\n",
      "Downloading 30 of 60 images\n",
      "Downloading 31 of 60 images\n",
      "Downloading 32 of 60 images\n",
      "Downloading 33 of 60 images\n",
      "Downloading 34 of 60 images\n",
      "Downloading 35 of 60 images\n",
      "Downloading 36 of 60 images\n",
      "Downloading 37 of 60 images\n",
      "Downloading 38 of 60 images\n",
      "Downloading 39 of 60 images\n",
      "Downloading 40 of 60 images\n",
      "Downloading 41 of 60 images\n",
      "Downloading 42 of 60 images\n",
      "Downloading 43 of 60 images\n",
      "Downloading 44 of 60 images\n",
      "Downloading 45 of 60 images\n",
      "Downloading 46 of 60 images\n",
      "Downloading 47 of 60 images\n",
      "Downloading 48 of 60 images\n",
      "Downloading 49 of 60 images\n",
      "Downloading 50 of 60 images\n",
      "Downloading 51 of 60 images\n",
      "Downloading 52 of 60 images\n",
      "Downloading 53 of 60 images\n",
      "Downloading 54 of 60 images\n",
      "Downloading 55 of 60 images\n",
      "Downloading 56 of 60 images\n",
      "Downloading 57 of 60 images\n",
      "Downloading 58 of 60 images\n",
      "Downloading 59 of 60 images\n",
      "Downloading 60 of 60 images\n",
      "Scraping of page 1 done\n",
      "Scraping page 2 of 3 pages\n",
      "The Current Page Scrapped is 2\n",
      "Downloading 1 of 60 images\n",
      "Downloading 2 of 60 images\n",
      "Downloading 3 of 60 images\n",
      "Downloading 4 of 60 images\n",
      "Downloading 5 of 60 images\n",
      "Downloading 6 of 60 images\n",
      "Downloading 7 of 60 images\n",
      "Downloading 8 of 60 images\n",
      "Downloading 9 of 60 images\n",
      "Downloading 10 of 60 images\n",
      "Downloading 11 of 60 images\n",
      "Downloading 12 of 60 images\n",
      "Downloading 13 of 60 images\n",
      "Downloading 14 of 60 images\n",
      "Downloading 15 of 60 images\n",
      "Downloading 16 of 60 images\n",
      "Downloading 17 of 60 images\n",
      "Downloading 18 of 60 images\n",
      "Downloading 19 of 60 images\n",
      "Downloading 20 of 60 images\n",
      "Downloading 21 of 60 images\n",
      "Downloading 22 of 60 images\n",
      "Downloading 23 of 60 images\n",
      "Downloading 24 of 60 images\n",
      "Downloading 25 of 60 images\n",
      "Downloading 26 of 60 images\n",
      "Downloading 27 of 60 images\n",
      "Downloading 28 of 60 images\n",
      "Downloading 29 of 60 images\n",
      "Downloading 30 of 60 images\n",
      "Downloading 31 of 60 images\n",
      "Downloading 32 of 60 images\n",
      "Downloading 33 of 60 images\n",
      "Downloading 34 of 60 images\n",
      "Downloading 35 of 60 images\n",
      "Downloading 36 of 60 images\n",
      "Downloading 37 of 60 images\n",
      "Downloading 38 of 60 images\n",
      "Downloading 39 of 60 images\n",
      "Downloading 40 of 60 images\n",
      "Downloading 41 of 60 images\n",
      "Downloading 42 of 60 images\n",
      "Downloading 43 of 60 images\n",
      "Downloading 44 of 60 images\n",
      "Downloading 45 of 60 images\n",
      "Downloading 46 of 60 images\n",
      "Downloading 47 of 60 images\n",
      "Downloading 48 of 60 images\n",
      "Downloading 49 of 60 images\n",
      "Downloading 50 of 60 images\n",
      "Downloading 51 of 60 images\n",
      "Downloading 52 of 60 images\n",
      "Downloading 53 of 60 images\n",
      "Downloading 54 of 60 images\n",
      "Downloading 55 of 60 images\n",
      "Downloading 56 of 60 images\n",
      "Downloading 57 of 60 images\n",
      "Downloading 58 of 60 images\n",
      "Downloading 59 of 60 images\n",
      "Downloading 60 of 60 images\n",
      "Scraping of page 2 done\n",
      "Scraping page 3 of 3 pages\n",
      "The Current Page Scrapped is 2\n",
      "Downloading 1 of 60 images\n",
      "Downloading 2 of 60 images\n",
      "Downloading 3 of 60 images\n",
      "Downloading 4 of 60 images\n",
      "Downloading 5 of 60 images\n",
      "Downloading 6 of 60 images\n",
      "Downloading 7 of 60 images\n",
      "Downloading 8 of 60 images\n",
      "Downloading 9 of 60 images\n",
      "Downloading 10 of 60 images\n",
      "Downloading 11 of 60 images\n",
      "Downloading 12 of 60 images\n",
      "Downloading 13 of 60 images\n",
      "Downloading 14 of 60 images\n",
      "Downloading 15 of 60 images\n",
      "Downloading 16 of 60 images\n",
      "Downloading 17 of 60 images\n",
      "Downloading 18 of 60 images\n",
      "Downloading 19 of 60 images\n",
      "Downloading 20 of 60 images\n",
      "Downloading 21 of 60 images\n",
      "Downloading 22 of 60 images\n",
      "Downloading 23 of 60 images\n",
      "Downloading 24 of 60 images\n",
      "Downloading 25 of 60 images\n",
      "Downloading 26 of 60 images\n",
      "Downloading 27 of 60 images\n",
      "Downloading 28 of 60 images\n",
      "Downloading 29 of 60 images\n",
      "Downloading 30 of 60 images\n",
      "Downloading 31 of 60 images\n",
      "Downloading 32 of 60 images\n",
      "Downloading 33 of 60 images\n",
      "Downloading 34 of 60 images\n",
      "Downloading 35 of 60 images\n",
      "Downloading 36 of 60 images\n",
      "Downloading 37 of 60 images\n",
      "Downloading 38 of 60 images\n",
      "Downloading 39 of 60 images\n",
      "Downloading 40 of 60 images\n",
      "Downloading 41 of 60 images\n",
      "Downloading 42 of 60 images\n",
      "Downloading 43 of 60 images\n",
      "Downloading 44 of 60 images\n",
      "Downloading 45 of 60 images\n",
      "Downloading 46 of 60 images\n",
      "Downloading 47 of 60 images\n",
      "Downloading 48 of 60 images\n",
      "Downloading 49 of 60 images\n",
      "Downloading 50 of 60 images\n",
      "Downloading 51 of 60 images\n",
      "Downloading 52 of 60 images\n",
      "Downloading 53 of 60 images\n",
      "Downloading 54 of 60 images\n",
      "Downloading 55 of 60 images\n",
      "Downloading 56 of 60 images\n",
      "Downloading 57 of 60 images\n",
      "Downloading 58 of 60 images\n",
      "Downloading 59 of 60 images\n",
      "Downloading 60 of 60 images\n",
      "Scraping of page 3 done\n"
     ]
    }
   ],
   "source": [
    "for page in range(start_page, total_pages+1):\n",
    "    try:\n",
    "        product=scrap_image_url(driver=driver)\n",
    "        print(\"Scraping page {0} of {1} pages\".format(page,total_pages))\n",
    "         \n",
    "        page_value=driver.find_element_by_xpath(\"//li[@class='a-normal']//a\").text\n",
    "        print('The Current Page Scrapped is {}'.format(page_value))\n",
    "        \n",
    "        #download the images\n",
    "        save_images(data=product,dirname='Amazon_men_Tshirts',page=page)\n",
    "        print(\"Scraping of page {0} done\".format(page))\n",
    "        \n",
    "    except StaleElementReferenceException as Exception:\n",
    "        print('We are facing an exception')\n",
    "        \n",
    "        exp_page=driver.find_element_by_xpath(\"//li[@class='a-normal']//a\").text\n",
    "        print('The page value at the time of exception is {}'.format(exp_page))\n",
    "        \n",
    "        value=driver.find_element_by_xpath(\"//li[@class='a-normal']//a\")\n",
    "        link=value.get_attribute('href')\n",
    "        driver.get(link)\n",
    "        \n",
    "        product=scrap_image_url(driver=driver)\n",
    "        print(\"Scraping page {0} of {1} pages\".format(page,total_pages))\n",
    "         \n",
    "        page_value=driver.find_element_by_xpath(\"//li[@class='a-normal']//a\").text\n",
    "        print('The Current Page Scrapped is {}'.format(page_value))\n",
    "        \n",
    "        #download the images\n",
    "        save_images(data=product,dirname='Amazon_men_Tshirts',page=page)\n",
    "        print(\"Scraping of page {0} done\".format(page))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrapped images from Amazon(sarees(women))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the libraries\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import StaleElementReferenceException\n",
    "import os\n",
    "import shutil\n",
    "import requests\n",
    "\n",
    "#getting the url of webpage\n",
    "\n",
    "driver=webdriver.Chrome('C:\\\\Users\\\\hp\\\\Downloads\\\\chromedriver_win32\\\\chromedriver.exe')\n",
    "my_page=driver.get('https://www.amazon.in/s/ref=lp_1968253031_nr_n_0?fst=as%3Aoff&rh=n%3A1571271031%2Cn%3A%211571272031%2Cn%3A1953602031%2Cn%3A1968253031%2Cn%3A1968256031&bbn=1968253031&ie=UTF8&qid=1593952433&rnid=1968253031')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make directory to store scrapped images\n",
    "\n",
    "def make_directory(dirname):\n",
    "    current_path=os.getcwd()\n",
    "    path=os.path.join(current_path,dirname)\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_directory('Amazon_women_sarees')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scraping images of further pages\n",
    "\n",
    "start_page=1\n",
    "total_pages=3\n",
    "\n",
    "def scrap_image_url(driver):\n",
    "    images=driver.find_elements_by_xpath(\"//div[@class='a-row a-gesture a-gesture-horizontal']//img\")\n",
    "    product_data= {}\n",
    "    product_data['image_urls']=[]\n",
    "    for image in images:\n",
    "        source=image.get_attribute('src')\n",
    "        product_data['image_urls'].append(source)\n",
    "    return product_data\n",
    "\n",
    "\n",
    "def save_images(data,dirname,page):\n",
    "    for index,link in enumerate(data['image_urls']):\n",
    "        print('Downloading {0} of {1} images'.format(index+1,len(data['image_urls'])))\n",
    "        response=requests.get(link)\n",
    "        with open('{0}/img_{1}{2}.jpeg'.format(dirname,page,index),'wb') as file:\n",
    "            file.write(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page 1 of 3 pages\n",
      "The Current Page Scrapped is 2\n",
      "Downloading 1 of 38 images\n",
      "Downloading 2 of 38 images\n",
      "Downloading 3 of 38 images\n",
      "Downloading 4 of 38 images\n",
      "Downloading 5 of 38 images\n",
      "Downloading 6 of 38 images\n",
      "Downloading 7 of 38 images\n",
      "Downloading 8 of 38 images\n",
      "Downloading 9 of 38 images\n",
      "Downloading 10 of 38 images\n",
      "Downloading 11 of 38 images\n",
      "Downloading 12 of 38 images\n",
      "Downloading 13 of 38 images\n",
      "Downloading 14 of 38 images\n",
      "Downloading 15 of 38 images\n",
      "Downloading 16 of 38 images\n",
      "Downloading 17 of 38 images\n",
      "Downloading 18 of 38 images\n",
      "Downloading 19 of 38 images\n",
      "Downloading 20 of 38 images\n",
      "Downloading 21 of 38 images\n",
      "Downloading 22 of 38 images\n",
      "Downloading 23 of 38 images\n",
      "Downloading 24 of 38 images\n",
      "Downloading 25 of 38 images\n",
      "Downloading 26 of 38 images\n",
      "Downloading 27 of 38 images\n",
      "Downloading 28 of 38 images\n",
      "Downloading 29 of 38 images\n",
      "Downloading 30 of 38 images\n",
      "Downloading 31 of 38 images\n",
      "Downloading 32 of 38 images\n",
      "Downloading 33 of 38 images\n",
      "Downloading 34 of 38 images\n",
      "Downloading 35 of 38 images\n",
      "Downloading 36 of 38 images\n",
      "Downloading 37 of 38 images\n",
      "Downloading 38 of 38 images\n",
      "Scraping of page 1 done\n",
      "Scraping page 2 of 3 pages\n",
      "The Current Page Scrapped is 2\n",
      "Downloading 1 of 38 images\n",
      "Downloading 2 of 38 images\n",
      "Downloading 3 of 38 images\n",
      "Downloading 4 of 38 images\n",
      "Downloading 5 of 38 images\n",
      "Downloading 6 of 38 images\n",
      "Downloading 7 of 38 images\n",
      "Downloading 8 of 38 images\n",
      "Downloading 9 of 38 images\n",
      "Downloading 10 of 38 images\n",
      "Downloading 11 of 38 images\n",
      "Downloading 12 of 38 images\n",
      "Downloading 13 of 38 images\n",
      "Downloading 14 of 38 images\n",
      "Downloading 15 of 38 images\n",
      "Downloading 16 of 38 images\n",
      "Downloading 17 of 38 images\n",
      "Downloading 18 of 38 images\n",
      "Downloading 19 of 38 images\n",
      "Downloading 20 of 38 images\n",
      "Downloading 21 of 38 images\n",
      "Downloading 22 of 38 images\n",
      "Downloading 23 of 38 images\n",
      "Downloading 24 of 38 images\n",
      "Downloading 25 of 38 images\n",
      "Downloading 26 of 38 images\n",
      "Downloading 27 of 38 images\n",
      "Downloading 28 of 38 images\n",
      "Downloading 29 of 38 images\n",
      "Downloading 30 of 38 images\n",
      "Downloading 31 of 38 images\n",
      "Downloading 32 of 38 images\n",
      "Downloading 33 of 38 images\n",
      "Downloading 34 of 38 images\n",
      "Downloading 35 of 38 images\n",
      "Downloading 36 of 38 images\n",
      "Downloading 37 of 38 images\n",
      "Downloading 38 of 38 images\n",
      "Scraping of page 2 done\n",
      "Scraping page 3 of 3 pages\n",
      "The Current Page Scrapped is 2\n",
      "Downloading 1 of 38 images\n",
      "Downloading 2 of 38 images\n",
      "Downloading 3 of 38 images\n",
      "Downloading 4 of 38 images\n",
      "Downloading 5 of 38 images\n",
      "Downloading 6 of 38 images\n",
      "Downloading 7 of 38 images\n",
      "Downloading 8 of 38 images\n",
      "Downloading 9 of 38 images\n",
      "Downloading 10 of 38 images\n",
      "Downloading 11 of 38 images\n",
      "Downloading 12 of 38 images\n",
      "Downloading 13 of 38 images\n",
      "Downloading 14 of 38 images\n",
      "Downloading 15 of 38 images\n",
      "Downloading 16 of 38 images\n",
      "Downloading 17 of 38 images\n",
      "Downloading 18 of 38 images\n",
      "Downloading 19 of 38 images\n",
      "Downloading 20 of 38 images\n",
      "Downloading 21 of 38 images\n",
      "Downloading 22 of 38 images\n",
      "Downloading 23 of 38 images\n",
      "Downloading 24 of 38 images\n",
      "Downloading 25 of 38 images\n",
      "Downloading 26 of 38 images\n",
      "Downloading 27 of 38 images\n",
      "Downloading 28 of 38 images\n",
      "Downloading 29 of 38 images\n",
      "Downloading 30 of 38 images\n",
      "Downloading 31 of 38 images\n",
      "Downloading 32 of 38 images\n",
      "Downloading 33 of 38 images\n",
      "Downloading 34 of 38 images\n",
      "Downloading 35 of 38 images\n",
      "Downloading 36 of 38 images\n",
      "Downloading 37 of 38 images\n",
      "Downloading 38 of 38 images\n",
      "Scraping of page 3 done\n"
     ]
    }
   ],
   "source": [
    "for page in range(start_page, total_pages+1):\n",
    "    try:\n",
    "        product=scrap_image_url(driver=driver)\n",
    "        print(\"Scraping page {0} of {1} pages\".format(page,total_pages))\n",
    "         \n",
    "        page_value=driver.find_element_by_xpath(\"//span[@class='pagnLink']//a\").text\n",
    "        print('The Current Page Scrapped is {}'.format(page_value))\n",
    "        \n",
    "        #download the images\n",
    "        save_images(data=product,dirname='Amazon_women_sarees',page=page)\n",
    "        print(\"Scraping of page {0} done\".format(page))\n",
    "        \n",
    "    except StaleElementReferenceException as Exception:\n",
    "        print('We are facing an exception')\n",
    "        \n",
    "        exp_page=driver.find_element_by_xpath(\"//span[@class='pagnLink']//a\").text\n",
    "        print('The page value at the time of exception is {}'.format(exp_page))\n",
    "        \n",
    "        value=driver.find_element_by_xpath(\"//span[@class='pagnLink']//a\")\n",
    "        link=value.get_attribute('href')\n",
    "        driver.get(link)\n",
    "        \n",
    "        product=scrap_image_url(driver=driver)\n",
    "        print(\"Scraping page {0} of {1} pages\".format(page,total_pages))\n",
    "         \n",
    "        page_value=driver.find_element_by_xpath(\"//span[@class='pagnLink']//a\").text\n",
    "        print('The Current Page Scrapped is {}'.format(page_value))\n",
    "        \n",
    "        #download the images\n",
    "        save_images(data=product,dirname='Amazon_women_sarees',page=page)\n",
    "        print(\"Scraping of page {0} done\".format(page))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting keras==1.1.0\n",
      "  Downloading Keras-1.1.0.tar.gz (150 kB)\n",
      "Collecting theano\n",
      "  Downloading Theano-1.0.4.tar.gz (2.8 MB)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\hp\\anaconda3\\envs\\virtual\\lib\\site-packages (from keras==1.1.0) (5.3.1)\n",
      "Requirement already satisfied: six in c:\\users\\hp\\anaconda3\\envs\\virtual\\lib\\site-packages (from keras==1.1.0) (1.15.0)\n",
      "Requirement already satisfied: numpy>=1.9.1 in c:\\users\\hp\\anaconda3\\envs\\virtual\\lib\\site-packages (from theano->keras==1.1.0) (1.18.5)\n",
      "Requirement already satisfied: scipy>=0.14 in c:\\users\\hp\\anaconda3\\envs\\virtual\\lib\\site-packages (from theano->keras==1.1.0) (1.5.0)\n",
      "Building wheels for collected packages: keras, theano\n",
      "  Building wheel for keras (setup.py): started\n",
      "  Building wheel for keras (setup.py): finished with status 'done'\n",
      "  Created wheel for keras: filename=Keras-1.1.0-py3-none-any.whl size=178691 sha256=77fc118a3adce51121b43f7db9c01bdd51a16cecdd8d6d6d7de8a9abbb75b99f\n",
      "  Stored in directory: c:\\users\\hp\\appdata\\local\\pip\\cache\\wheels\\87\\14\\fb\\5b08fcc312aadd1ed933e557cdb78ab1f30ec9e203f3acdd5f\n",
      "  Building wheel for theano (setup.py): started\n",
      "  Building wheel for theano (setup.py): finished with status 'done'\n",
      "  Created wheel for theano: filename=Theano-1.0.4-py3-none-any.whl size=2667194 sha256=05e2c6b0f173c40a29d38165b7e4469c6e4bc3ab2148ca072d3bac682ea5fa77\n",
      "  Stored in directory: c:\\users\\hp\\appdata\\local\\pip\\cache\\wheels\\22\\6b\\c8\\952faef457482ca081255c4a887f44f11490d1175743559aad\n",
      "Successfully built keras theano\n",
      "Installing collected packages: theano, keras\n",
      "  Attempting uninstall: keras\n",
      "    Found existing installation: Keras 2.4.3\n",
      "    Uninstalling Keras-2.4.3:\n",
      "      Successfully uninstalled Keras-2.4.3\n",
      "Successfully installed keras-1.1.0 theano-1.0.4\n"
     ]
    }
   ],
   "source": [
    "!pip install keras==1.1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\hp\\anaconda3\\envs\\Virtual\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:17: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#importing the libraries\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,Dropout,Activation,Flatten,Conv2D,MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "from keras import optimizers\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape=(150,150,3)\n",
    "img_width=150\n",
    "img_height=150\n",
    "\n",
    "train_samples=316\n",
    "validation_samples=122\n",
    "batch_size=32\n",
    "epochs=10\n",
    "\n",
    "train_data_dir=\"./Amazon Datasets/Train\"\n",
    "validation_data_dir=\"./Amazon Datasets/Validation\"\n",
    "\n",
    "#used to rescale the pixel values to (0,1) interval\n",
    "image_data_generator=ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 316 images belonging to 3 classes.\n",
      "Found 122 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "#automatically retrieve images and their classes for both train and test dataset\n",
    "train_generator=image_data_generator.flow_from_directory(\n",
    "        train_data_dir,\n",
    "        target_size=(img_width,img_height),\n",
    "        batch_size=16)\n",
    "\n",
    "validation_generator=image_data_generator.flow_from_directory(\n",
    "        validation_data_dir,\n",
    "        target_size=(img_width,img_height),\n",
    "        batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\hp\\anaconda3\\envs\\Virtual\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 148, 148, 32)      896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 74, 74, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 74, 74, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 72, 72, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 36, 36, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 36, 36, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 34, 34, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 17, 17, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 17, 17, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 18496)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               2367616   \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 20)                2580      \n",
      "=================================================================\n",
      "Total params: 2,398,836\n",
      "Trainable params: 2,398,836\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model=Sequential()\n",
    "\n",
    "model.add(Conv2D(32,(3,3),input_shape=input_shape,activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Conv2D(32,(3,3),activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Conv2D(64,(3,3),activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.2))\n",
    "    \n",
    "model.add(Flatten())\n",
    "model.add(Dense(128,activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(20,activation='softmax'))\n",
    "    \n",
    "print(model.summary())\n",
    "  \n",
    "model.compile(optimizer='adam',metrics=['accuracy'],loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pillow in c:\\users\\hp\\anaconda3\\envs\\virtual\\lib\\site-packages (7.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "A target array with shape (16, 3) was passed for an output of shape (None, 20) while using as loss `categorical_crossentropy`. This loss expects targets to have the same shape as the output.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-22a639975b91>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m             \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m             \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_generator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m             validation_steps=validation_samples // batch_size)\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\Virtual\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   1294\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1295\u001b[0m         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1296\u001b[1;33m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[0;32m   1297\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1298\u001b[0m   def evaluate_generator(self,\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Virtual\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_generator.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[1;34m(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, steps_name, **kwargs)\u001b[0m\n\u001b[0;32m    263\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    264\u001b[0m       \u001b[0mis_deferred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_compiled\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 265\u001b[1;33m       \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mbatch_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    266\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    267\u001b[0m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Virtual\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[0;32m    989\u001b[0m     x, y, sample_weights = self._standardize_user_data(\n\u001b[0;32m    990\u001b[0m         \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 991\u001b[1;33m         extract_tensors_from_dataset=True)\n\u001b[0m\u001b[0;32m    992\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    993\u001b[0m     \u001b[1;31m# If `self._distribution_strategy` is True, then we are in a replica context\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Virtual\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[0;32m   2535\u001b[0m           \u001b[1;31m# Additional checks to avoid users mistakenly using improper loss fns.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2536\u001b[0m           training_utils.check_loss_and_target_compatibility(\n\u001b[1;32m-> 2537\u001b[1;33m               y, self._feed_loss_fns, feed_output_shapes)\n\u001b[0m\u001b[0;32m   2538\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2539\u001b[0m       \u001b[1;31m# If sample weight mode has not been set and weights are None for all the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Virtual\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mcheck_loss_and_target_compatibility\u001b[1;34m(targets, loss_fns, output_shapes)\u001b[0m\n\u001b[0;32m    739\u001b[0m           raise ValueError('A target array with shape ' + str(y.shape) +\n\u001b[0;32m    740\u001b[0m                            \u001b[1;34m' was passed for an output of shape '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 741\u001b[1;33m                            \u001b[1;34m' while using as loss `'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mloss_name\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'`. '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    742\u001b[0m                            \u001b[1;34m'This loss expects targets to have the same shape '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    743\u001b[0m                            'as the output.')\n",
      "\u001b[1;31mValueError\u001b[0m: A target array with shape (16, 3) was passed for an output of shape (None, 20) while using as loss `categorical_crossentropy`. This loss expects targets to have the same shape as the output."
     ]
    }
   ],
   "source": [
    "history=model.fit_generator(\n",
    "            train_generator,\n",
    "            steps_per_epoch=train_samples // batch_size,\n",
    "            epochs=epochs,\n",
    "            validation_data=validation_generator,\n",
    "            validation_steps=validation_samples // batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
